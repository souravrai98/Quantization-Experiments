{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf94c7c2",
   "metadata": {
    "id": "bf94c7c2"
   },
   "source": [
    "## Custom Quantization functions and classses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e43fd0",
   "metadata": {
    "id": "e8e43fd0"
   },
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a0fe68",
   "metadata": {
    "id": "98a0fe68"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import models, layers, utils\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f478a",
   "metadata": {
    "id": "cd3f478a"
   },
   "source": [
    "### Functions for quantizing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10870183",
   "metadata": {
    "id": "10870183"
   },
   "outputs": [],
   "source": [
    "\n",
    "## This is the function for linear quantization and the steps for quantization are explained in the report \n",
    "def lin_quant(tensor, n_bits):\n",
    "    q_min_val = -(2 ** (n_bits - 1))\n",
    "    q_max_val = (2 ** (n_bits - 1)) - 1\n",
    "    scl = (np.max(tensor) - np.min(tensor)) / (q_max_val - q_min_val)\n",
    "    zero_pt = q_min_val - np.round(np.min(tensor) / scl)\n",
    "    q_tensor = np.round(tensor / scl) - zero_pt\n",
    "    q_tensor = np.clip(q_tensor, q_min_val, q_max_val).astype(np.int32)\n",
    "    dq_tensor = (q_tensor + zero_pt) * scl\n",
    "    return q_tensor, dq_tensor \n",
    "\n",
    "## This function is used to just quantize the biases in the network\n",
    "def lin_quant_bias(tnsr):\n",
    "    n_bits = 32\n",
    "    q_min_val = -(2 ** (n_bits - 1))\n",
    "    q_max_val = (2 ** (n_bits - 1)) - 1\n",
    "    scl = (np.max(tnsr) - np.min(tnsr)) / (q_max_val - q_min_val)\n",
    "    zero_pt = q_min_val - np.round(np.min(tnsr) / scl)\n",
    "    q_tnsr = np.round(tnsr / scl) - zero_pt\n",
    "    q_tnsr = np.clip(q_tnsr, q_min_val, q_max_val).astype(np.int32)\n",
    "    dq_tnsr = (q_tnsr + zero_pt) * scl\n",
    "    return q_tnsr, dq_tnsr\n",
    "\n",
    "## This is a weighted quantization method. The expalantion for this method is included in the report\n",
    "def non_uni_quant(tnsr, n_bits, centrs=None):\n",
    "    if centrs is None:\n",
    "        centrs = np.linspace(np.min(tnsr), np.max(tnsr), num=2 ** n_bits)\n",
    "    \n",
    "    q_tnsr = np.zeros_like(tnsr, dtype=np.int32)\n",
    "    for i in range(2 ** n_bits):\n",
    "        if i == 0:\n",
    "            msk = np.abs(tnsr - centrs[i]) <= np.abs(tnsr - centrs[i + 1])\n",
    "        elif i == 2 ** n_bits - 1:\n",
    "            msk = np.abs(tnsr - centrs[i]) < np.abs(tnsr - centrs[i - 1])\n",
    "        else:\n",
    "            msk = (np.abs(tnsr - centrs[i]) < np.abs(tnsr - centrs[i - 1])) & \\\n",
    "                   (np.abs(tnsr - centrs[i]) <= np.abs(tnsr - centrs[i + 1]))\n",
    "        q_tnsr[msk] = i\n",
    "\n",
    "    dq_tnsr = np.array([centrs[i] for i in q_tnsr])\n",
    "    return q_tnsr, dq_tnsr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f67930c",
   "metadata": {
    "id": "8f67930c"
   },
   "outputs": [],
   "source": [
    "# Quantize just the weights not the biases\n",
    "def quantize_model_weights(model, quantization_function, num_bits):\n",
    "    quantized_weights = []\n",
    "    dequantized_weights = []\n",
    "    for weight in model.get_weights():\n",
    "        if len(weight.shape) > 1:\n",
    "            q_weight, deq_weight = quantization_function(weight, num_bits)\n",
    "            quantized_weights.append(q_weight)\n",
    "            dequantized_weights.append(deq_weight)\n",
    "        else:\n",
    "            quantized_weights.append(weight)\n",
    "            dequantized_weights.append(weight)  # Keep biases unchanged\n",
    "    return quantized_weights, dequantized_weights\n",
    "\n",
    "# Quantize the weights alongwith the biases\n",
    "def quantize_model_weights_biases(model, quantization_function, num_bits):\n",
    "    quantized_weights = []\n",
    "    dequantized_weights = []\n",
    "    for weight in model.get_weights():\n",
    "        q_weight, deq_weight = quantization_function(weight, num_bits)\n",
    "        quantized_weights.append(q_weight)\n",
    "        dequantized_weights.append(deq_weight)\n",
    "    return quantized_weights, dequantized_weights\n",
    "\n",
    "# Clone the floating point model and then replace the weights of the original model \n",
    "def replace_all_weights(model, quantized_weights,actual_loss = 'categorical_crossentropy'):\n",
    "    quantized_model = tf.keras.models.clone_model(model)\n",
    "    quantized_model.set_weights(quantized_weights)\n",
    "    quantized_model.compile(optimizer='adam',\n",
    "                        loss=actual_loss,\n",
    "                        metrics=['accuracy'])\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e813556",
   "metadata": {
    "id": "3e813556"
   },
   "source": [
    "### Function for Measuring Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d312ad82",
   "metadata": {
    "id": "d312ad82"
   },
   "outputs": [],
   "source": [
    "def measure_inference_time(model_instance, input_data, iterations=100):\n",
    "    \n",
    "    # Warm up the cache by executing the model for one step\n",
    "    model_instance.predict(input_data, verbose=0)\n",
    "\n",
    "# Measure the prediction time\n",
    "    start_timestamp = time.time()\n",
    "    for _ in range(iterations):\n",
    "        model_instance.predict(input_data, verbose=0)\n",
    "    end_timestamp = time.time()\n",
    "\n",
    "# Calculate and return the average prediction time per run\n",
    "    average_duration = (end_timestamp - start_timestamp) / iterations\n",
    "    return average_duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b34be4",
   "metadata": {
    "id": "a9b34be4"
   },
   "source": [
    "**We make two additional function to load the tflite model and measure its inference time. Since predicting using a tflite model is less starightforward than a .h5 model. We usually need to use an interpreter for evaluation. Tensorflow doesn't provide a simple way to predict using the tflite model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966f0979",
   "metadata": {
    "id": "966f0979"
   },
   "outputs": [],
   "source": [
    "def measure_tflite_inference_time(model_filepath, input_data, runs=100):\n",
    "    \n",
    "   # A Tflite model requires an interpreter for evaluation\n",
    "    tflite_evaluator = Interpreter(model_path=model_filepath)\n",
    "    tflite_evaluator.allocate_tensors()\n",
    "\n",
    "    i_info = tflite_evaluator.get_input_details()\n",
    "    o_info = tflite_evaluator.get_output_details()\n",
    "\n",
    "    # Warm up the cache by executing the model for one step\n",
    "    data_input = np.array(input_data, dtype=i_info[0]['dtype'])\n",
    "    tflite_evaluator.set_tensor(i_info[0]['index'], data_input)\n",
    "    tflite_evaluator.invoke()\n",
    "\n",
    "    # Measure the execution duration\n",
    "    start_timestamp = time.time()\n",
    "    for _ in range(runs):\n",
    "        tflite_evaluator.set_tensor(i_info[0]['index'], data_input)\n",
    "        tflite_evaluator.invoke()\n",
    "    end_timestamp = time.time()\n",
    "\n",
    "    # Calculate and return the average execution time per run\n",
    "    avg_duration = (end_timestamp - start_timestamp) / runs\n",
    "    return avg_duration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da04a6",
   "metadata": {
    "id": "43da04a6"
   },
   "source": [
    "### Function for calculating Accuracy of a TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64284ce",
   "metadata": {
    "id": "d64284ce"
   },
   "outputs": [],
   "source": [
    "def evaluate_tflite_model_accuracy(model_filepath, test_data_x, test_data_y):\n",
    "# A Tflite model needs an interpreter for evaluation\n",
    "    tflite_evaluator = Interpreter(model_path=model_filepath)\n",
    "    tflite_evaluator.allocate_tensors()\n",
    "    i_spec = tflite_evaluator.get_input_details()\n",
    "    o_spec = tflite_evaluator.get_output_details()\n",
    "\n",
    "# Prepare data for evaluation\n",
    "    # Prepare data for evaluation\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(test_data_x)\n",
    "\n",
    "    for idx in range(total_samples):\n",
    "        input_sample = np.array(test_data_x[idx:idx+1], dtype=i_spec[0]['dtype'])\n",
    "        tflite_evaluator.set_tensor(i_spec[0]['index'], input_sample)\n",
    "        tflite_evaluator.invoke()\n",
    "        output_sample = tflite_evaluator.get_tensor(o_spec[0]['index'])\n",
    "\n",
    "    # Get the predicted label and compare with the ground truth label\n",
    "        estimated_label = np.argmax(output_sample)\n",
    "        actual_label = np.argmax(test_data_y[idx])\n",
    "\n",
    "        if estimated_label == actual_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "# Calculate and return the accuracy\n",
    "    model_precision = correct_predictions / total_samples\n",
    "    return model_precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c809f5",
   "metadata": {
    "id": "87c809f5"
   },
   "source": [
    "## Testing our functions on a very simple model on MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311ae029",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "311ae029",
    "outputId": "9f4f1253-7584-4bf5-940a-24ba506617a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.2902 - accuracy: 0.9179 - val_loss: 0.1651 - val_accuracy: 0.9532\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1309 - accuracy: 0.9622 - val_loss: 0.1166 - val_accuracy: 0.9644\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0902 - accuracy: 0.9734 - val_loss: 0.0978 - val_accuracy: 0.9725\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0656 - accuracy: 0.9809 - val_loss: 0.0919 - val_accuracy: 0.9721\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Test accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(Training_Data, Training_Labels), (Testing_Data, Testing_Labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "Training_Data = Training_Data.reshape(-1, 784) / 255.0  # Reshape and normalize input data\n",
    "Testing_Data = Testing_Data.reshape(-1, 784) / 255.0  # Reshape and normalize input data\n",
    "\n",
    "# A very simple model\n",
    "inputs = tf.keras.layers.Input(shape=(784,))\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "Simple_Model = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Compile and train the model\n",
    "Simple_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "Simple_Model.fit(Training_Data, Training_Labels, epochs=4, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = Simple_Model.evaluate(Testing_Data, Testing_Labels)\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d429da4",
   "metadata": {
    "id": "2d429da4"
   },
   "source": [
    "## Comparing the Quantization accuracy for the Simple Model using our Qunatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d8038ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d8038ea",
    "outputId": "9beb9d51-b863-41a5-c967-292edd4903ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for 2 bit quantization\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Original Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1483 - accuracy: 0.6598\n",
      "Linear Dequantized Model Test Accuracy: 0.66\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.9969 - accuracy: 0.4021\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.40\n",
      "Accuracies for 4 bit quantization\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Original Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0942 - accuracy: 0.9710\n",
      "Linear Dequantized Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.9744\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.97\n",
      "Accuracies for 8 bit quantization\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Original Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0847 - accuracy: 0.9732\n",
      "Linear Dequantized Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.9734\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.97\n",
      "Accuracies for 16 bit quantization\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Original Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0847 - accuracy: 0.9736\n",
      "Linear Dequantized Model Test Accuracy: 0.97\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.97\n",
      "  num_bits  Original  Linear  Non_Uniform  Original_Size_MB  Linear_Size_MB  \\\n",
      "0        2    0.9739  0.6598       0.4021            1.1893          0.4045   \n",
      "1        4    0.9739  0.9710       0.9744            1.1893          0.4045   \n",
      "2        8    0.9739  0.9732       0.9734            1.1893          0.4045   \n",
      "3       16    0.9739  0.9736       0.9739            1.1893          0.4045   \n",
      "\n",
      "   Non_Uniform_Size_MB  Original_Time (s)  Linear_Time (s)  \\\n",
      "0               0.4045           0.051519         0.043343   \n",
      "1               0.4045           0.044754         0.057887   \n",
      "2               0.4045           0.052747         0.045022   \n",
      "3               0.4045           0.044959         0.044896   \n",
      "\n",
      "   Non_Uniform_Time (s)  \n",
      "0              0.048597  \n",
      "1              0.045995  \n",
      "2              0.063848  \n",
      "3              0.056113  \n"
     ]
    }
   ],
   "source": [
    "num_bits = [2,4,8,16]\n",
    "Simple_results = pd.DataFrame(columns=['num_bits', 'Original', 'Linear', 'Non_Uniform',\n",
    "                                         'Original_Size_MB', 'Linear_Size_MB', 'Non_Uniform_Size_MB',\n",
    "                                         'Original_Time (s)', 'Linear_Time (s)', 'Non_Uniform_Time (s)'])\n",
    "\n",
    "\n",
    "for num_bit in num_bits:\n",
    "    print(f\"Accuracies for {num_bit} bit quantization\")\n",
    "    quantized_weights_linear, dequantized_weights_linear = quantize_model_weights(Simple_Model, lin_quant, num_bit)\n",
    "    quantized_weights_non_uniform, dequantized_weights_non_uniform = quantize_model_weights(Simple_Model, non_uni_quant, num_bit)\n",
    "\n",
    "    # Evaluate the original model\n",
    "    loss, original_accuracy = Simple_Model.evaluate(Testing_Data, Testing_Labels)\n",
    "    print(\"Original Model Test Accuracy: {:.2f}\".format(original_accuracy))\n",
    "\n",
    "    # Evaluate the model with linear quantization\n",
    "    simple_model_linear = replace_all_weights(Simple_Model, dequantized_weights_linear, actual_loss='sparse_categorical_crossentropy')\n",
    "    loss, linear_accuracy = simple_model_linear.evaluate(Testing_Data, Testing_Labels)\n",
    "    print(\"Linear Dequantized Model Test Accuracy: {:.2f}\".format(linear_accuracy))\n",
    "\n",
    "    # Evaluate the model with non-uniform quantization\n",
    "    simple_model_non_uniform = replace_all_weights(Simple_Model, dequantized_weights_non_uniform, actual_loss='sparse_categorical_crossentropy')\n",
    "    loss, non_uniform_accuracy = simple_model_non_uniform.evaluate(Testing_Data, Testing_Labels)\n",
    "    print(\"Non-Uniform Dequantized Model Test Accuracy: {:.2f}\".format(non_uniform_accuracy))\n",
    "\n",
    "    # Save the models\n",
    "    simple_model_linear.save(f'simple_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    simple_model_non_uniform.save(f'simple_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Get the size of the models\n",
    "    linear_size = os.path.getsize(f'simple_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    non_uniform_size = os.path.getsize(f'simple_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Prepare the test data\n",
    "    test_data = Testing_Data[:1]\n",
    "\n",
    "    # Measure the inference time of the models\n",
    "    original_model_time = measure_inference_time(Simple_Model, test_data)\n",
    "    linear_quantized_time = measure_inference_time(simple_model_linear, test_data)\n",
    "    non_uniform_quantized_time = measure_inference_time(simple_model_non_uniform, test_data)\n",
    "\n",
    "    # Save the original model\n",
    "    Simple_Model.save('Simple_model_original.h5')\n",
    "\n",
    "    # Get the size of the original model\n",
    "    original_size = os.path.getsize('Simple_model_original.h5')\n",
    "\n",
    "     # Append the results to the DataFrame\n",
    "    new_row = pd.DataFrame({'num_bits': [num_bit],\n",
    "                            'Original': [round(original_accuracy, 4)],\n",
    "                            'Linear': [round(linear_accuracy, 4)],\n",
    "                            'Non_Uniform': [round(non_uniform_accuracy, 4)],\n",
    "                            'Original_Size_MB': [round(original_size / (1024 * 1024), 4)],\n",
    "                            'Linear_Size_MB': [round(linear_size / (1024 * 1024), 4)],\n",
    "                            'Non_Uniform_Size_MB': [round(non_uniform_size / (1024 * 1024), 4)],\n",
    "                            'Original_Time (s)': [round(original_model_time, 6)],\n",
    "                            'Linear_Time (s)': [round(linear_quantized_time, 6)],\n",
    "                            'Non_Uniform_Time (s)': [round(non_uniform_quantized_time, 6)]})\n",
    "\n",
    "    \n",
    "    Simple_results = pd.concat([Simple_results, new_row], ignore_index=True)\n",
    "    \n",
    "print(Simple_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "844494a1",
   "metadata": {
    "id": "844494a1"
   },
   "outputs": [],
   "source": [
    "file_name = 'Accuracy_Simple_model.csv'\n",
    "Simple_results.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d81c9",
   "metadata": {
    "id": "f28d81c9"
   },
   "source": [
    "## Quantizing with the Official Tensorflow Quantization Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44cfb33f",
   "metadata": {
    "id": "44cfb33f"
   },
   "outputs": [],
   "source": [
    "# Full 8-bit Integer Quantization\n",
    "eight_bit_converter = tf.lite.TFLiteConverter.from_keras_model(Simple_Model)\n",
    "eight_bit_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_quantized_model = eight_bit_converter.convert()\n",
    "\n",
    "# Quantized Tensorflow Model Final Save in tflite Format\n",
    "with open(\"Simple_quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c82a",
   "metadata": {
    "id": "9ae6c82a"
   },
   "source": [
    "## I now load the tensorflow quantized model and check it's size , inference time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025e2fa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025e2fa1",
    "outputId": "e4286cda-215b-49f2-abca-ca07f3fba380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Size: 0.10 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the quantized TensorFlow Lite model\n",
    "quantized_model_path = \"Simple_quantized_model.tflite\"\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "# Get the size of the quantized model\n",
    "quantized_model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Print the size of the quantized model in MB\n",
    "print(\"Quantized Model Size: {:.2f} MB\".format(quantized_model_size / (1024 * 1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24178ebe",
   "metadata": {
    "id": "24178ebe"
   },
   "source": [
    "**We can observe from the results above that the tensorflow quantized model is 0.1 MB whereas with our quantization the size was 0.4 MB. And these two values are still smaller than the original size of 1.19 MB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c860b575",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c860b575",
    "outputId": "c71c9296-f9ff-409b-95d5-af340b28b247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Average Inference Time: 0.000018 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the inference time of the Tensorflow Quantized model\n",
    "quantized_original_model_time = measure_tflite_inference_time(quantized_model_path, test_data)\n",
    "print(\"Quantized Model Average Inference Time: {:.6f} seconds\".format(quantized_original_model_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5bc21f",
   "metadata": {
    "id": "ab5bc21f"
   },
   "source": [
    "**The TFLite quantized inference time is much less than our implementation i.e 0.000018 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb391c2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb391c2b",
    "outputId": "fdf8b330-ebf4-4b95-adb5-4a4a5f6303b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the TFLite model: 9.99%\n"
     ]
    }
   ],
   "source": [
    "accuracy_simple_quantized = evaluate_tflite_model_accuracy(quantized_model_path, Testing_Data, Testing_Labels)\n",
    "print(f\"Accuracy of the TFLite model: {accuracy_simple_quantized  * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52be3",
   "metadata": {
    "id": "a8d52be3"
   },
   "source": [
    "**TFLite quantized model has a signifiacant degradation in accuracy here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8c757",
   "metadata": {
    "id": "5ed8c757"
   },
   "source": [
    "# Now we test our functions on a Large CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73ba06",
   "metadata": {
    "id": "ba73ba06"
   },
   "source": [
    "## Model to test our quantization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d337e4cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d337e4cd",
    "outputId": "4f4e7110-24ed-4b83-e204-b145b7a58113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n",
      "Epoch 1/40\n",
      "1563/1563 [==============================] - 26s 10ms/step - loss: 1.6301 - accuracy: 0.4396 - val_loss: 1.1670 - val_accuracy: 0.5929\n",
      "Epoch 2/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.0914 - accuracy: 0.6141 - val_loss: 0.9045 - val_accuracy: 0.6852\n",
      "Epoch 3/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.9451 - accuracy: 0.6707 - val_loss: 0.8537 - val_accuracy: 0.6985\n",
      "Epoch 4/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.8638 - accuracy: 0.7034 - val_loss: 0.7870 - val_accuracy: 0.7205\n",
      "Epoch 5/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.8028 - accuracy: 0.7217 - val_loss: 0.6981 - val_accuracy: 0.7556\n",
      "Epoch 6/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7511 - accuracy: 0.7391 - val_loss: 0.6198 - val_accuracy: 0.7870\n",
      "Epoch 7/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.7180 - accuracy: 0.7533 - val_loss: 0.6136 - val_accuracy: 0.7844\n",
      "Epoch 8/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6877 - accuracy: 0.7633 - val_loss: 0.7033 - val_accuracy: 0.7601\n",
      "Epoch 9/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6716 - accuracy: 0.7710 - val_loss: 0.6908 - val_accuracy: 0.7613\n",
      "Epoch 10/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.6405 - accuracy: 0.7815 - val_loss: 0.7426 - val_accuracy: 0.7475\n",
      "Epoch 11/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.6186 - accuracy: 0.7893 - val_loss: 0.6344 - val_accuracy: 0.7799\n",
      "Epoch 12/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6088 - accuracy: 0.7924 - val_loss: 0.5332 - val_accuracy: 0.8182\n",
      "Epoch 13/40\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5923 - accuracy: 0.7987 - val_loss: 0.6273 - val_accuracy: 0.7858\n",
      "Epoch 14/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5740 - accuracy: 0.8051 - val_loss: 0.5061 - val_accuracy: 0.8277\n",
      "Epoch 15/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5635 - accuracy: 0.8080 - val_loss: 0.5485 - val_accuracy: 0.8147\n",
      "Epoch 16/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5513 - accuracy: 0.8119 - val_loss: 0.5023 - val_accuracy: 0.8279\n",
      "Epoch 17/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5434 - accuracy: 0.8138 - val_loss: 0.4832 - val_accuracy: 0.8364\n",
      "Epoch 18/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5268 - accuracy: 0.8195 - val_loss: 0.5324 - val_accuracy: 0.8187\n",
      "Epoch 19/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5228 - accuracy: 0.8208 - val_loss: 0.4412 - val_accuracy: 0.8482\n",
      "Epoch 20/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.5132 - accuracy: 0.8255 - val_loss: 0.4412 - val_accuracy: 0.8493\n",
      "Epoch 21/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5105 - accuracy: 0.8253 - val_loss: 0.4738 - val_accuracy: 0.8383\n",
      "Epoch 22/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4980 - accuracy: 0.8300 - val_loss: 0.4678 - val_accuracy: 0.8386\n",
      "Epoch 23/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4894 - accuracy: 0.8326 - val_loss: 0.4521 - val_accuracy: 0.8449\n",
      "Epoch 24/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4855 - accuracy: 0.8336 - val_loss: 0.4394 - val_accuracy: 0.8554\n",
      "Epoch 25/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4818 - accuracy: 0.8373 - val_loss: 0.4396 - val_accuracy: 0.8533\n",
      "Epoch 26/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4745 - accuracy: 0.8364 - val_loss: 0.4558 - val_accuracy: 0.8452\n",
      "Epoch 27/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4692 - accuracy: 0.8378 - val_loss: 0.4621 - val_accuracy: 0.8461\n",
      "Epoch 28/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4626 - accuracy: 0.8430 - val_loss: 0.4185 - val_accuracy: 0.8569\n",
      "Epoch 29/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4540 - accuracy: 0.8454 - val_loss: 0.4318 - val_accuracy: 0.8527\n",
      "Epoch 30/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4533 - accuracy: 0.8446 - val_loss: 0.4494 - val_accuracy: 0.8498\n",
      "Epoch 31/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4452 - accuracy: 0.8472 - val_loss: 0.4145 - val_accuracy: 0.8613\n",
      "Epoch 32/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4401 - accuracy: 0.8503 - val_loss: 0.4302 - val_accuracy: 0.8575\n",
      "Epoch 33/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4370 - accuracy: 0.8495 - val_loss: 0.4171 - val_accuracy: 0.8595\n",
      "Epoch 34/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4330 - accuracy: 0.8511 - val_loss: 0.4293 - val_accuracy: 0.8567\n",
      "Epoch 35/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4271 - accuracy: 0.8540 - val_loss: 0.4559 - val_accuracy: 0.8482\n",
      "Epoch 36/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4203 - accuracy: 0.8557 - val_loss: 0.4299 - val_accuracy: 0.8565\n",
      "Epoch 37/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4181 - accuracy: 0.8578 - val_loss: 0.4145 - val_accuracy: 0.8632\n",
      "Epoch 38/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4170 - accuracy: 0.8589 - val_loss: 0.4195 - val_accuracy: 0.8627\n",
      "Epoch 39/40\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.4128 - accuracy: 0.8593 - val_loss: 0.4108 - val_accuracy: 0.8612\n",
      "Epoch 40/40\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4058 - accuracy: 0.8613 - val_loss: 0.4029 - val_accuracy: 0.8661\n",
      "Test loss: 0.402946799993515\n",
      "Test accuracy: 0.866100013256073\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess CIFAR-10 data\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = cifar10.load_data()\n",
    "\n",
    "#Normalization of the training and test values \n",
    "Xtrain = Xtrain.astype('float32') / 255\n",
    "Xtest = Xtest.astype('float32') / 255\n",
    "\n",
    "# Encoding the labels to vectors of one and zero\n",
    "Ytrain = utils.to_categorical(Ytrain, 10)\n",
    "Ytest = utils.to_categorical(Ytest, 10)\n",
    "\n",
    "\n",
    "# Model for CIFAR-10 dataset\n",
    "Large_CNN_Model = models.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "Large_CNN_Model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Training Part\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "training_of_the_model = Large_CNN_Model.fit(\n",
    "    x=Xtrain, y=Ytrain, batch_size=batch_size,\n",
    "    epochs=epochs, validation_data=(Xtest, Ytest)\n",
    ")\n",
    "\n",
    "# Model Evaluation Part\n",
    "score = Large_CNN_Model.evaluate(Xtest, Ytest, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97863414",
   "metadata": {
    "id": "97863414"
   },
   "source": [
    "### Comparing the accuracies, inference time and file size of the Large CNN model using our quantization functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4f21de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d4f21de",
    "outputId": "96538843-b897-41de-bee1-79fb9e010832",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for 2 bit quantization\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4029 - accuracy: 0.8661\n",
      "Original Model Test Accuracy: 0.87\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 6.4498 - accuracy: 0.1008\n",
      "Linear Dequantized Model Test Accuracy: 0.10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 23.8798 - accuracy: 0.1312\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.13\n",
      "Accuracies for 4 bit quantization\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4029 - accuracy: 0.8661\n",
      "Original Model Test Accuracy: 0.87\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.5262 - accuracy: 0.8237\n",
      "Linear Dequantized Model Test Accuracy: 0.82\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.4569 - accuracy: 0.8435\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.84\n",
      "Accuracies for 8 bit quantization\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.4029 - accuracy: 0.8661\n",
      "Original Model Test Accuracy: 0.87\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.4077 - accuracy: 0.8633\n",
      "Linear Dequantized Model Test Accuracy: 0.86\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.4024 - accuracy: 0.8656\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.87\n",
      "Accuracies for 16 bit quantization\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4029 - accuracy: 0.8661\n",
      "Original Model Test Accuracy: 0.87\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.4093 - accuracy: 0.8629\n",
      "Linear Dequantized Model Test Accuracy: 0.86\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.4029 - accuracy: 0.8661\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.87\n",
      "  num_bits  Original  Linear  Non_Uniform  Original_Size_MB  Linear_Size_MB  \\\n",
      "0        2    0.8661  0.1008       0.1312            6.4538          2.1926   \n",
      "1        4    0.8661  0.8237       0.8435            6.4538          2.1926   \n",
      "2        8    0.8661  0.8633       0.8656            6.4538          2.1926   \n",
      "3       16    0.8661  0.8629       0.8661            6.4538          2.1926   \n",
      "\n",
      "   Non_Uniform_Size_MB  Original_Time (s)  Linear_Time (s)  \\\n",
      "0               2.1926           0.048365         0.055125   \n",
      "1               2.1926           0.048562         0.053049   \n",
      "2               2.1926           0.051655         0.048543   \n",
      "3               2.1926           0.047659         0.046539   \n",
      "\n",
      "   Non_Uniform_Time (s)  \n",
      "0              0.054465  \n",
      "1              0.046443  \n",
      "2              0.052348  \n",
      "3              0.052771  \n",
      "Original Model Size: 6.45 MB\n"
     ]
    }
   ],
   "source": [
    "Large_CNN_results = pd.DataFrame(columns=['num_bits', 'Original', 'Linear', 'Non_Uniform',\n",
    "                                         'Original_Size_MB', 'Linear_Size_MB', 'Non_Uniform_Size_MB',\n",
    "                                         'Original_Time (s)', 'Linear_Time (s)', 'Non_Uniform_Time (s)'])\n",
    "\n",
    "num_bits = [2, 4, 8, 16]\n",
    "\n",
    "for num_bit in num_bits:\n",
    "    print(f\"Accuracies for {num_bit} bit quantization\")\n",
    "    quantized_weights_linear, dequantized_weights_linear = quantize_model_weights(Large_CNN_Model, lin_quant, num_bit)\n",
    "    quantized_weights_non_uniform, dequantized_weights_non_uniform = quantize_model_weights(Large_CNN_Model, non_uni_quant, num_bit)\n",
    "\n",
    "    # Evaluate the original model\n",
    "    loss, original_accuracy = Large_CNN_Model.evaluate(Xtest, Ytest)\n",
    "    print(\"Original Model Test Accuracy: {:.2f}\".format(original_accuracy))\n",
    "\n",
    "    # Evaluate the model with linear quantization\n",
    "    Large_model_linear = replace_all_weights(Large_CNN_Model, dequantized_weights_linear, actual_loss='categorical_crossentropy')\n",
    "    loss, linear_accuracy = Large_model_linear.evaluate(Xtest, Ytest)\n",
    "    print(\"Linear Dequantized Model Test Accuracy: {:.2f}\".format(linear_accuracy))\n",
    "\n",
    "    # Evaluate the model with non-uniform quantization\n",
    "    Large_model_non_uniform = replace_all_weights(Large_CNN_Model, dequantized_weights_non_uniform, actual_loss='categorical_crossentropy')\n",
    "    loss, non_uniform_accuracy = Large_model_non_uniform.evaluate(Xtest, Ytest)\n",
    "    print(\"Non-Uniform Dequantized Model Test Accuracy: {:.2f}\".format(non_uniform_accuracy))\n",
    "\n",
    "    # Save the models\n",
    "    Large_model_linear.save(f'Large_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    Large_model_non_uniform.save(f'Large_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Get the size of the models\n",
    "    linear_size = os.path.getsize(f'Large_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    non_uniform_size = os.path.getsize(f'Large_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Prepare the test data\n",
    "    test_data = Xtest[:1]\n",
    "\n",
    "    # Measure the inference time of the models\n",
    "    original_model_time = measure_inference_time(Large_CNN_Model, test_data)\n",
    "    linear_quantized_time = measure_inference_time(Large_model_linear, test_data)\n",
    "    non_uniform_quantized_time = measure_inference_time(Large_model_non_uniform, test_data)\n",
    "\n",
    "\n",
    "    # Save the original model\n",
    "    Large_CNN_Model.save('Large_CNN_model_original.h5')\n",
    "\n",
    "    # Get the size of the original model\n",
    "    original_size = os.path.getsize('Large_CNN_model_original.h5')\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = pd.DataFrame({'num_bits': [num_bit],\n",
    "                            'Original': [round(original_accuracy, 4)],\n",
    "                            'Linear': [round(linear_accuracy, 4)],\n",
    "                            'Non_Uniform': [round(non_uniform_accuracy, 4)],\n",
    "                            'Original_Size_MB': [round(original_size / (1024 * 1024), 4)],\n",
    "                            'Linear_Size_MB': [round(linear_size / (1024 * 1024), 4)],\n",
    "                            'Non_Uniform_Size_MB': [round(non_uniform_size / (1024 * 1024), 4)],\n",
    "                            'Original_Time (s)': [round(original_model_time, 6)],\n",
    "                            'Linear_Time (s)': [round(linear_quantized_time, 6)],\n",
    "                            'Non_Uniform_Time (s)': [round(non_uniform_quantized_time, 6)]})\n",
    "\n",
    "    \n",
    "    Large_CNN_results = pd.concat([Large_CNN_results, new_row], ignore_index=True)\n",
    "    \n",
    "print(Large_CNN_results)\n",
    "\n",
    "# Print the size of the original model\n",
    "\n",
    "print('Original Model Size: {:.2f} MB'.format(original_size / (1024 * 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35b16a1",
   "metadata": {
    "id": "c35b16a1"
   },
   "outputs": [],
   "source": [
    "file_name = 'Accuracy_Large_CNN_model.csv'\n",
    "Large_CNN_results.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca07e5",
   "metadata": {
    "id": "34ca07e5"
   },
   "source": [
    "## Quantizing the Large CNN model using TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c4b479c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c4b479c",
    "outputId": "ed0b84ae-0766-45dc-e58a-a5b9c990b71e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# Full 8-bit Integer Quantization\n",
    "eight_bit_converter = tf.lite.TFLiteConverter.from_keras_model(Large_CNN_Model)\n",
    "eight_bit_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_quantized_model = eight_bit_converter.convert()\n",
    "\n",
    "# Quantized Tensorflow Model Final Save in tflite Format\n",
    "with open(\"Large_quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b4dbe",
   "metadata": {
    "id": "155b4dbe"
   },
   "source": [
    "### Size, Inference Times and the Latency of the TFLite quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5bf21eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5bf21eb",
    "outputId": "0252437f-75ef-4ffe-f5c2-14ff478d0232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Size: 0.55 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the quantized TensorFlow Lite model\n",
    "quantized_model_path = \"Large_quantized_model.tflite\"\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "# Get the size of the quantized model\n",
    "quantized_model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Print the size of the quantized model in MB\n",
    "print(\"Quantized Model Size: {:.2f} MB\".format(quantized_model_size / (1024 * 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34ac195d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ac195d",
    "outputId": "ad9e87cb-3e06-4e3f-deac-a18bf4af35dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Average Inference Time: 0.002209 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the inference time of the Tensorflow Quantized model\n",
    "quantized_original_model_time = measure_tflite_inference_time(quantized_model_path, test_data)\n",
    "print(\"Quantized Model Average Inference Time: {:.6f} seconds\".format(quantized_original_model_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b638e643",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b638e643",
    "outputId": "0d04c717-caa9-4840-9f9b-ab530d68976e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the TFLite model: 86.59%\n"
     ]
    }
   ],
   "source": [
    "accuracy_simple_quantized = evaluate_tflite_model_accuracy(quantized_model_path, Xtest, Ytest)\n",
    "print(f\"Accuracy of the TFLite model: {accuracy_simple_quantized  * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91427eb2",
   "metadata": {
    "id": "91427eb2"
   },
   "source": [
    "## Testing our functions on bigger models like ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe174149",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe174149",
    "outputId": "3bcf4a8e-245c-4a5b-98c6-67cce27da316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "196/196 [==============================] - 62s 111ms/step - loss: 1.9102 - accuracy: 0.3726 - val_loss: 3.4312 - val_accuracy: 0.1062\n",
      "Epoch 2/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 1.4949 - accuracy: 0.4787 - val_loss: 3.8793 - val_accuracy: 0.1460\n",
      "Epoch 3/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 1.2757 - accuracy: 0.5551 - val_loss: 1.4732 - val_accuracy: 0.4833\n",
      "Epoch 4/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.1225 - accuracy: 0.6080 - val_loss: 1.3654 - val_accuracy: 0.5207\n",
      "Epoch 5/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 1.0969 - accuracy: 0.6270 - val_loss: 1.1834 - val_accuracy: 0.5824\n",
      "Epoch 6/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.1494 - accuracy: 0.6154 - val_loss: 1.4113 - val_accuracy: 0.5043\n",
      "Epoch 7/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 1.0536 - accuracy: 0.6468 - val_loss: 1.1976 - val_accuracy: 0.5816\n",
      "Epoch 8/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.0779 - accuracy: 0.6322 - val_loss: 1.4671 - val_accuracy: 0.5030\n",
      "Epoch 9/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.9201 - accuracy: 0.6920 - val_loss: 1.4320 - val_accuracy: 0.5347\n",
      "Epoch 10/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.3199 - accuracy: 0.6033 - val_loss: 496154.1250 - val_accuracy: 0.1000\n",
      "Epoch 11/40\n",
      "196/196 [==============================] - 18s 93ms/step - loss: 1.4087 - accuracy: 0.5393 - val_loss: 19.2462 - val_accuracy: 0.3633\n",
      "Epoch 12/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 1.2632 - accuracy: 0.5889 - val_loss: 2.5380 - val_accuracy: 0.3291\n",
      "Epoch 13/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.0176 - accuracy: 0.6705 - val_loss: 1.2129 - val_accuracy: 0.5813\n",
      "Epoch 14/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.8883 - accuracy: 0.7170 - val_loss: 1.3729 - val_accuracy: 0.5546\n",
      "Epoch 15/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.7883 - accuracy: 0.7436 - val_loss: 1.5913 - val_accuracy: 0.5177\n",
      "Epoch 16/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.9661 - accuracy: 0.6788 - val_loss: 1.2598 - val_accuracy: 0.5859\n",
      "Epoch 17/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.7315 - accuracy: 0.7578 - val_loss: 1.2779 - val_accuracy: 0.5863\n",
      "Epoch 18/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.6113 - accuracy: 0.8000 - val_loss: 2.0715 - val_accuracy: 0.6638\n",
      "Epoch 19/40\n",
      "196/196 [==============================] - 18s 93ms/step - loss: 0.5147 - accuracy: 0.8315 - val_loss: 1.8029 - val_accuracy: 0.5613\n",
      "Epoch 20/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.5226 - accuracy: 0.8299 - val_loss: 6.5056 - val_accuracy: 0.2821\n",
      "Epoch 21/40\n",
      "196/196 [==============================] - 18s 93ms/step - loss: 1.5775 - accuracy: 0.4652 - val_loss: 2.0956 - val_accuracy: 0.3832\n",
      "Epoch 22/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 1.1898 - accuracy: 0.5915 - val_loss: 1.2543 - val_accuracy: 0.5523\n",
      "Epoch 23/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 1.0899 - accuracy: 0.6289 - val_loss: 1.6614 - val_accuracy: 0.4718\n",
      "Epoch 24/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.9449 - accuracy: 0.6766 - val_loss: 1.3823 - val_accuracy: 0.5433\n",
      "Epoch 25/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.8766 - accuracy: 0.7038 - val_loss: 238.0616 - val_accuracy: 0.1502\n",
      "Epoch 26/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.8700 - accuracy: 0.7197 - val_loss: 1.2898 - val_accuracy: 0.5601\n",
      "Epoch 27/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.6901 - accuracy: 0.7698 - val_loss: 0.9488 - val_accuracy: 0.6802\n",
      "Epoch 28/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.6419 - accuracy: 0.7875 - val_loss: 1.1472 - val_accuracy: 0.6359\n",
      "Epoch 29/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.5776 - accuracy: 0.8177 - val_loss: 4.1189 - val_accuracy: 0.6144\n",
      "Epoch 30/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.4585 - accuracy: 0.8500 - val_loss: 1.1266 - val_accuracy: 0.6937\n",
      "Epoch 31/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.4316 - accuracy: 0.8620 - val_loss: 1.1150 - val_accuracy: 0.6689\n",
      "Epoch 32/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.3472 - accuracy: 0.8859 - val_loss: 2.1575 - val_accuracy: 0.5347\n",
      "Epoch 33/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.4951 - accuracy: 0.8498 - val_loss: 2.3973 - val_accuracy: 0.6166\n",
      "Epoch 34/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.3896 - accuracy: 0.8850 - val_loss: 1.8060 - val_accuracy: 0.6427\n",
      "Epoch 35/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.7853 - accuracy: 0.7586 - val_loss: 9.9334 - val_accuracy: 0.4613\n",
      "Epoch 36/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.6265 - accuracy: 0.7847 - val_loss: 0.9199 - val_accuracy: 0.6995\n",
      "Epoch 37/40\n",
      "196/196 [==============================] - 19s 96ms/step - loss: 0.5029 - accuracy: 0.8301 - val_loss: 2.6693 - val_accuracy: 0.4169\n",
      "Epoch 38/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.4385 - accuracy: 0.8465 - val_loss: 1.1656 - val_accuracy: 0.6640\n",
      "Epoch 39/40\n",
      "196/196 [==============================] - 18s 94ms/step - loss: 0.2642 - accuracy: 0.9076 - val_loss: 1.0988 - val_accuracy: 0.7050\n",
      "Epoch 40/40\n",
      "196/196 [==============================] - 19s 95ms/step - loss: 0.1634 - accuracy: 0.9436 - val_loss: 1.5055 - val_accuracy: 0.6724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a60787f70>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess CIFAR-10 data\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#Normalization of the training and test values \n",
    "Xtrain = Xtrain.astype('float32') / 255\n",
    "Xtest = Xtest.astype('float32') / 255\n",
    "\n",
    "# One hot encoding the labels or we can use sparse_categorical_crossentropy as our loss function\n",
    "Ytrain = utils.to_categorical(Ytrain, 10)\n",
    "Ytest = utils.to_categorical(Ytest, 10)\n",
    "\n",
    "\n",
    "# Training a ResNet50 model\n",
    "ResNet_50_model = ResNet50(weights=None, include_top=True, input_shape=(32, 32, 3), classes=10)\n",
    "ResNet_50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Train the model\n",
    "ResNet_50_model.fit(Xtrain, Ytrain, epochs=40,batch_size = batch_size,validation_data=(Xtest, Ytest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a189ad4",
   "metadata": {
    "id": "0a189ad4"
   },
   "source": [
    "### Quantized Accuracy using our funtions for ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24902244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24902244",
    "outputId": "77723afd-d700-4685-bf10-d3009f2b5eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for 2 bit quantization\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 1.5055 - accuracy: 0.6724\n",
      "Original Model Test Accuracy: 0.67\n",
      "313/313 [==============================] - 5s 10ms/step - loss: 2.9564 - accuracy: 0.1000\n",
      "Linear Dequantized Model Test Accuracy: 0.10\n",
      "313/313 [==============================] - 5s 12ms/step - loss: 2.8102 - accuracy: 0.1000\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.10\n",
      "Accuracies for 4 bit quantization\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 1.5055 - accuracy: 0.6724\n",
      "Original Model Test Accuracy: 0.67\n",
      "313/313 [==============================] - 5s 11ms/step - loss: 2.1551 - accuracy: 0.5758\n",
      "Linear Dequantized Model Test Accuracy: 0.58\n",
      "313/313 [==============================] - 5s 11ms/step - loss: 1.7271 - accuracy: 0.6363\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.64\n",
      "Accuracies for 8 bit quantization\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.5055 - accuracy: 0.6724\n",
      "Original Model Test Accuracy: 0.67\n",
      "313/313 [==============================] - 5s 11ms/step - loss: 1.5186 - accuracy: 0.6693\n",
      "Linear Dequantized Model Test Accuracy: 0.67\n",
      "313/313 [==============================] - 5s 11ms/step - loss: 1.5155 - accuracy: 0.6712\n",
      "Non-Uniform Dequantized Model Test Accuracy: 0.67\n",
      "  num_bits  Original  Linear  Non_Uniform  Original_Size_MB  Linear_Size_MB  \\\n",
      "0        2    0.6724  0.1000       0.1000          270.6505         90.5555   \n",
      "1        4    0.6724  0.5758       0.6363          270.6505         90.5555   \n",
      "2        8    0.6724  0.6693       0.6712          270.6505         90.5555   \n",
      "\n",
      "   Non_Uniform_Size_MB  Original_Time (s)  Linear_Time (s)  \\\n",
      "0              90.5555           0.061806         0.061334   \n",
      "1              90.5555           0.074481         0.061416   \n",
      "2              90.5555           0.065523         0.061083   \n",
      "\n",
      "   Non_Uniform_Time (s)  \n",
      "0              0.065794  \n",
      "1              0.077213  \n",
      "2              0.067622  \n",
      "Original Model Size: 270.65 MB\n"
     ]
    }
   ],
   "source": [
    "Resnet_results = pd.DataFrame(columns=['num_bits', 'Original', 'Linear', 'Non_Uniform',\n",
    "                                         'Original_Size_MB', 'Linear_Size_MB', 'Non_Uniform_Size_MB',\n",
    "                                         'Original_Time (s)', 'Linear_Time (s)', 'Non_Uniform_Time (s)'])\n",
    "\n",
    "num_bits = [2, 4, 8]\n",
    "\n",
    "for num_bit in num_bits:\n",
    "    print(f\"Accuracies for {num_bit} bit quantization\")\n",
    "    quantized_weights_linear, dequantized_weights_linear = quantize_model_weights(ResNet_50_model, lin_quant, num_bit)\n",
    "    quantized_weights_non_uniform, dequantized_weights_non_uniform = quantize_model_weights(ResNet_50_model, non_uni_quant, num_bit)\n",
    "\n",
    "    # Evaluate the original model\n",
    "    loss, original_accuracy = ResNet_50_model.evaluate(Xtest, Ytest)\n",
    "    print(\"Original Model Test Accuracy: {:.2f}\".format(original_accuracy))\n",
    "\n",
    "    # Evaluate the model with linear quantization\n",
    "    Resnet_model_linear = replace_all_weights(ResNet_50_model, dequantized_weights_linear, actual_loss='categorical_crossentropy')\n",
    "    loss, linear_accuracy = Resnet_model_linear.evaluate(Xtest, Ytest)\n",
    "    print(\"Linear Dequantized Model Test Accuracy: {:.2f}\".format(linear_accuracy))\n",
    "\n",
    "    # Evaluate the model with non-uniform quantization\n",
    "    Resnet_model_non_uniform = replace_all_weights(ResNet_50_model, dequantized_weights_non_uniform, actual_loss='categorical_crossentropy')\n",
    "    loss, non_uniform_accuracy = Resnet_model_non_uniform.evaluate(Xtest, Ytest)\n",
    "    print(\"Non-Uniform Dequantized Model Test Accuracy: {:.2f}\".format(non_uniform_accuracy))\n",
    "\n",
    "    # Save the models\n",
    "    Resnet_model_linear.save(f'Large_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    Resnet_model_non_uniform.save(f'Large_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Get the size of the models\n",
    "    linear_size = os.path.getsize(f'Large_model_linear_quantized_{num_bit}_bits.h5')\n",
    "    non_uniform_size = os.path.getsize(f'Large_model_non_uniform_quantized_{num_bit}_bits.h5')\n",
    "\n",
    "    # Prepare the test data\n",
    "    test_data = Xtest[:1]\n",
    "\n",
    "    # Measure the inference time of the models\n",
    "    original_model_time = measure_inference_time(ResNet_50_model, test_data)\n",
    "    linear_quantized_time = measure_inference_time(Resnet_model_linear, test_data)\n",
    "    non_uniform_quantized_time = measure_inference_time(Resnet_model_non_uniform, test_data)\n",
    "    # Save the original model\n",
    "    ResNet_50_model.save('Resnet_model_original.h5')\n",
    "\n",
    "    # Get the size of the original model\n",
    "    original_size = os.path.getsize('Resnet_model_original.h5')\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = pd.DataFrame({'num_bits': [num_bit],\n",
    "                            'Original': [round(original_accuracy, 4)],\n",
    "                            'Linear': [round(linear_accuracy, 4)],\n",
    "                            'Non_Uniform': [round(non_uniform_accuracy, 4)],\n",
    "                            'Original_Size_MB': [round(original_size / (1024 * 1024), 4)],\n",
    "                            'Linear_Size_MB': [round(linear_size / (1024 * 1024), 4)],\n",
    "                            'Non_Uniform_Size_MB': [round(non_uniform_size / (1024 * 1024), 4)],\n",
    "                            'Original_Time (s)': [round(original_model_time, 6)],\n",
    "                            'Linear_Time (s)': [round(linear_quantized_time, 6)],\n",
    "                            'Non_Uniform_Time (s)': [round(non_uniform_quantized_time, 6)]})\n",
    "\n",
    "    \n",
    "    Resnet_results = pd.concat([Resnet_results, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "print(Resnet_results)\n",
    "\n",
    "# Print the size of the original model\n",
    "\n",
    "print('Original Model Size: {:.2f} MB'.format(original_size / (1024 * 1024)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f500305",
   "metadata": {
    "id": "8f500305"
   },
   "outputs": [],
   "source": [
    "file_name = 'Accuracy_Resnet_model.csv'\n",
    "Resnet_results.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ecde12",
   "metadata": {
    "id": "53ecde12"
   },
   "source": [
    "## Quantizing the ResNet model using TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d6008f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d6008f4",
    "outputId": "678ee0d4-2dff-4230-a466-61938a7ee9c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your Keras model\n",
    "resnet_model = tf.keras.models.load_model(\"Resnet_model_original.h5\")\n",
    "\n",
    "# 8-bit quantized model TFLite\n",
    "eight_converter = tf.lite.TFLiteConverter.from_keras_model(resnet_model)\n",
    "eight_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_quantized_model = eight_converter.convert()\n",
    "\n",
    "# Save the quantized TensorFlow Lite model\n",
    "with open(\"Resnet_quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f724a2",
   "metadata": {
    "id": "70f724a2"
   },
   "source": [
    "### Size, Inference Times and the Accuracy of the TFLite quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37b673d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37b673d1",
    "outputId": "19650baa-56cd-4c37-aa5b-c811b49668c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Size: 22.84 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the quantized TensorFlow Lite model\n",
    "quantized_model_path = \"Resnet_quantized_model.tflite\"\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "# Get the size of the quantized model\n",
    "quantized_model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Print the size of the quantized model in MB\n",
    "print(\"Quantized Model Size: {:.2f} MB\".format(quantized_model_size / (1024 * 1024)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b3bf91b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3bf91b",
    "outputId": "e70e77e7-9b61-4b87-f2cb-834ffbdde5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Average Inference Time: 0.011498 seconds\n"
     ]
    }
   ],
   "source": [
    "# Inference Time of the TFLite Quantized model\n",
    "quantized_resnet_model_time = measure_tflite_inference_time(quantized_model_path, test_data)\n",
    "print(\"Quantized Model Average Inference Time: {:.6f} seconds\".format(quantized_resnet_model_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a36b9403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a36b9403",
    "outputId": "d7d9078c-2ac1-440b-e643-7feed4579e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the TFLite model: 67.23%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for the tflite model\n",
    "accuracy_resnet_quantized = evaluate_tflite_model_accuracy(quantized_model_path, Xtest, Ytest)\n",
    "print(f\"Accuracy of the TFLite model: {accuracy_resnet_quantized  * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ffc8a",
   "metadata": {
    "id": "024ffc8a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
